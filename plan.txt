learn_rate = 0.01

Forward propogation:
1. hiddens = sigmoid(inputs * (input_hidden_bias + input_hidden_weights))
2. outputs = softmax(hiddens * (hidden_output_bias + hidden_output_weights))

Cost, error:
1. error = cross_entropy(true_values, outputs)

Back propogation:

1. //softmax der // o -> h
var delta_output (gradient_output) mat.Dense
delta_output.Sub(outputs, true_values)
var hiddens_adj mat.Dense
hiddens_adj.Scale(-learn_rate, delta_output)
hidden_output_bias.Add(hidden_output_bias, &hiddens_adj)
hiddens_adj.Mul(&hiddens_adj, hiddens.T())           
hidden_output_weights.Add(hidden_output_weights, &hiddens_adj)         

2. //sigmoid der // h -> i
var delta_hidden (gradient_hidden), hiddens_derivative, inputs_adj mat.Dense
hiddens_derivative = sigmoidDerivative(hiddens)
delta_hidden.Mul(hidden_output_weights.T(), delta_output)
delta_hidden.MulElem(delta_hidden, hiddens_derivative)

inputs_adj.Scale(-learn_rate, delta_hidden)
input_hidden_bias.Add(input_hidden_bias, &inputs_adj)
inputs_adj.Mul(inputs.T(), &inputs_adj)           
input_hidden_weights.Add(input_hidden_weights, &inputs_adj)         

Для обновления весов между входным и скрытым слоем нужно:

Δinput_hidden_weights = xᵀ × δ_hidden

Где:

    xᵀ: 3×1 (транспонированный вход)

    δ_hidden: 1×3 (ошибка скрытого слоя)

    Результат: 3×3 (размер весовой матрицы)